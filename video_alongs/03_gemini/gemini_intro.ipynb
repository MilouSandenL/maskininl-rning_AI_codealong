{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba24b4ed",
   "metadata": {},
   "source": [
    "# Gemini intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "360f454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  **What's a data engineer's favorite type of alarm clock?**\n",
      "    One that goes off at 3 AM to tell them a data pipeline failed.\n",
      "\n",
      "2.  **A data scientist walks up to a data engineer and asks, \"Can you just pull me that data?\"**\n",
      "    The data engineer sighs, \"There's no 'just' in data engineering.\"\n",
      "\n",
      "3.  **Why was the data engineer always calm during a fire drill?**\n",
      "    Because he knew nothing could be more catastrophic than an unexpected schema change.\n",
      "\n",
      "4.  **What's a data engineer's favorite household chore?**\n",
      "    Cleaning data. Because no matter how much you do, it'll never *really* be clean.\n",
      "\n",
      "5.  **What's a data engineer's spirit animal?**\n",
      "    A salmon, because they're constantly fighting upstream to get the data where it needs to go.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Generate some funny jokes about data engineering. Give 5 points\",\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daa4b583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62df076",
   "metadata": {},
   "source": [
    "## Analyze tokens\n",
    "\n",
    "- basic units of text for LLMs\n",
    "- can be short as one character or as long as one word\n",
    "\n",
    "The free tier in gemini API allows for (Gemini 2.5 flash)\n",
    "\n",
    "- Requesrs per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Requests per dat (RDP): 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a1ed88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=208,\n",
       "  prompt_token_count=13,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=13\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1855,\n",
       "  total_token_count=2076\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f7bd2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=208,\n",
       "  prompt_token_count=13,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=13\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1855,\n",
       "  total_token_count=2076\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "098170ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "isinstance(response, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40cbdba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output tokens - number of tokens in model response\n",
      "metadata.candidates_token_count = 208\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Output tokens - number of tokens in model response\")\n",
    "print(f\"{metadata.candidates_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24980a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in user input or user prompt\n",
      "metadata.prompt_token_count = 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens in user input or user prompt\")\n",
    "print(f\"{metadata.prompt_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e05a4922",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=208,\n",
       "  prompt_token_count=13,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=13\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1855,\n",
       "  total_token_count=2076\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d450659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens used for internal thinking\n",
      "metadata.thoughts_token_count = 1855\n"
     ]
    }
   ],
   "source": [
    "# a lot of tokens used here\n",
    "print(\"Tokens used for internal thinking\")\n",
    "print(f\"{metadata.thoughts_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cd8097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens used - this is the billing number\n",
      "metadata.total_token_count = 2076\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens used - this is the billing number\")\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef312a",
   "metadata": {},
   "source": [
    "## Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd411812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 funny jokes about data engineering:\n",
      "\n",
      "1.  Why did the data engineer break up with the API?\n",
      "    Because there were too many **missing fields** in their relationship!\n",
      "\n",
      "2.  What's a data engineer's favorite type of music?\n",
      "    **Stream**ing! (Especially when it's real-time.)\n",
      "\n",
      "3.  How many data engineers does it take to change a lightbulb?\n",
      "    None. They just declare a new variable `is_light_on = True`, run a Spark job to process the state change, and then wait for someone to build a dashboard to visualize it.\n",
      "\n",
      "4.  A data engineer walks into a bar. The bartender asks, \"What can I get you?\"\n",
      "    The data engineer replies, \"I'd like a beer, but can you make sure it's fully **normalized**, properly **partitioned**, and that I have read-only access to the source keg?\"\n",
      "\n",
      "5.  What do you call a data engineer who's always stressed?\n",
      "    Under **pressure** (to deliver those pipelines on time!).\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# The client gets the API key from the environment variable `GEMINI_API_KEY`.\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Generate some funny jokes about data engineering. Give 5 points\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fa01356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=232,\n",
      "  prompt_token_count=13,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=13\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=245\n",
      ")\n",
      "Ah much cheaper, but is the result as good as the thinking?\n"
     ]
    }
   ],
   "source": [
    "print(repr(response.usage_metadata))\n",
    "\n",
    "print(\"Ah much cheaper, but is the result as good as the thinking?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b42b07",
   "metadata": {},
   "source": [
    "## System instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c03cd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It\\'s the perfect weather to debug code outside. Just remember, sunny days are great for spotting errors, but dark nights are when the real \"bugs\" come out! ðŸ˜‰\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_instruction = \"\"\"You are a joking robot called Ro BÃ¥t, which \n",
    "        will always answer with a programming joke.\n",
    "        \"\"\"\n",
    "\n",
    "prompt = \"What is the weather today?\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    "    contents=prompt,\n",
    ")\n",
    "\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43894032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=37,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=37\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=30,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=30\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=67\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "549c8676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(system_instruction.split()) = 16\n",
      "len(prompt.split()) = 5\n",
      "plus some formatting overhead\n",
      "prompt token count 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(system_instruction.split()) = }\")\n",
    "print(f\"{len(prompt.split()) = }\")\n",
    "print(\"plus some formatting overhead\")\n",
    "\n",
    "print(f\"prompt token count {response.usage_metadata.prompt_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80798bf5",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec83a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes, its fluffy tail disappearing in a flash of white. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the mechanical threat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story_prompt = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "boring_story = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story_prompt,\n",
    "    config=types.GenerateContentConfig(temperature=0.0),\n",
    ")\n",
    "\n",
    "print(boring_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b78d5d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the fear that still lingered in its twitching whiskers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can see that the outputs are similar to the first when temperature is 0\n",
    "boring_story = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story_prompt,\n",
    "    config=types.GenerateContentConfig(temperature=0.0),\n",
    ")\n",
    "\n",
    "print(boring_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99fc20ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barnaby, a gray rabbit with a nervous twitch in his nose, hopped out of his burrow, sniffing the air cautiously. Today, however, the scent of sunshine and wildflowers was stronger than his fear, beckoning him toward an adventure beyond the familiar field.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "creative_story = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story_prompt,\n",
    "    config=types.GenerateContentConfig(temperature=2.0),\n",
    ")\n",
    "\n",
    "print(creative_story.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "626b1fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barnaby, a rabbit of unassuming gray, discovered a patch of glowing blue mushrooms nestled beneath a twisted oak. One nibble turned the forest floor into a symphony of color and Barnaby into a vibrant shade of electric blue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# very different story\n",
    "creative_story = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story_prompt,\n",
    "    config=types.GenerateContentConfig(temperature=2.0),\n",
    ")\n",
    "\n",
    "print(creative_story.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e88b0c3",
   "metadata": {},
   "source": [
    "## Multimodal inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84874db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a description of the dude, based on the image:\n",
      "\n",
      "*   **Appearance:** He's a young man with short, dark hair. He's wearing glasses with a square frame.\n",
      "\n",
      "*   **Outfit:** He's wearing a black and white raglan shirt. The shirt is printed with two graphics:\n",
      "    *   A bell curve labeled \"NORMAL DISTRIBUTION\"\n",
      "    *   Below that, a sad ghost-like shape that represents \"PARANORMAL DISTRIBUTION\".\n",
      "\n",
      "*   **Activity:** He's using a laptop with an Apple logo.\n",
      "\n",
      "*   **Expression:** He looks focused and a bit serious, perhaps deep in thought or concentrating on something on the screen.\n",
      "\n",
      "*   **Vibe:** He seems like a person who appreciates a good math joke or has an interest in statistics. He's possibly a student or someone working in a tech-related field.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents={\n",
    "        \"parts\": [\n",
    "            {\"text\": \"Tell me about this dude, write in markdown format.\"},\n",
    "            {\n",
    "                \"inline_data\": {\n",
    "                    \"mime_type\": \"image/png\",\n",
    "                    \"data\": open(\"assets/kokchun.png\", \"rb\").read(),\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59ea5371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"exports\").mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd4deb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exports/image_description.md\", \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maskininl-rning-ai-codealong",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
